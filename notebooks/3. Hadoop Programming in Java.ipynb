{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Programming in Java\n",
    "\n",
    "In this notebook we will detail the steps necessary to:\n",
    "\n",
    "0. [Install](#maven) Maven to manage the build process.\n",
    "1. [Develop](#write) an Hadoop program on your local machine.\n",
    "2. [Deploy](#deploy) and run an Hadoop program on yout Hadoop cluster.\n",
    "\n",
    "Before starting, run the following Jupyter cell to configure some scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:29:42.833236Z",
     "start_time": "2020-04-20T10:29:42.800857Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, on your local machine you must use the same Java version that is used on the Hadoop cluster. Since our Hadoop cluster was installed using Java 8, please make sure this Java 8 is installed in your local machine and the environment variable `JAVA_HOME` is correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:29:44.215297Z",
     "start_time": "2020-04-20T10:29:44.085767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home\r\n"
     ]
    }
   ],
   "source": [
    "!echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Hadoop and Maven <a name=\"maven\"/>\n",
    "\n",
    "For running a Hadoop job written in Java, we need to create a jar file with the compiled classes and also include other dependencies of our code, e.g., third-party libraries. This can be very time consuming if we do not automatise the tasks.\n",
    "\n",
    "[Apache Maven](http://maven.apache.org/) allows a project to build using its project object model (POM) and a set of plugins that are shared by all projects using Maven, providing a uniform build system.\n",
    "\n",
    "[Apache Maven](http://maven.apache.org/) is a **project management tool** which includes a project object model, a set of standards, a project lifecycle, a dependency management system, and\n",
    "the logic for executing plugin goals at defined phases in a lifecycle.\n",
    "\n",
    "When you use Maven, you describe your project using a well-defined **project object model** (the `pom.xml`file). Maven can then apply cross-cutting logic from a set of shared plugins.\n",
    "As a project management tool, Maven preprocesses, compiles, packages and tests you projects.\n",
    "\n",
    "If you want to install Maven on your virtual machines, please use the following command:\n",
    "\n",
    "```bash\n",
    "sudo apt-get install maven\n",
    "```\n",
    "\n",
    "If you want to install Maven on your laptop, use the same command on a Linux shell, or install the [Brew](https://brew.sh) packaging system on MacOS systems, then use the following command:\n",
    "\n",
    "```bash\n",
    "brew install maven\n",
    "```\n",
    "\n",
    "Now we will see how to configure a Apache Maven `pom.xml` file to obtain a single jar including our code plus the required dependencies ready to be executed on our Hadoop cluster.\n",
    "\n",
    "### Start with a Maven project archetype\n",
    "\n",
    "Archetype is a Maven project templating toolkit. We use Maven archetype to structure our Hadoop source code, to allow a quick and standardized development template.\n",
    "\n",
    "Using Maven to generate an archetype template we will get a project directory structure which we will populate wiuth our Java code and a `pom.xml` file that we will use to configure the build process.\n",
    "\n",
    "Before running the Maven archetype generation process, we need take three decisions:\n",
    "1. The **local directory** where we will develop our code. We just need to open a shell in that directory, where we will execute the following commands.\n",
    "2. The **artifact id** (`artifactId`) indicates the unique base name of the primary artifact being generated by this project. The primary artifact for a project is typically a JAR file. In the following, we will use the `wordcount` artifact id.\n",
    "3. The **group id** (`groupId`) indicates the unique identifier of the organization or group that creates the project. In the following, we will use the `it.unipi.hadoop` group id.\n",
    "\n",
    "We now can use Maven to generate our project archetype in our home directory with the following commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:29:48.616776Z",
     "start_time": "2020-04-20T10:29:48.606317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/khast\n"
     ]
    }
   ],
   "source": [
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:29:57.782013Z",
     "start_time": "2020-04-20T10:29:50.122445Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;34mINFO\u001b[m] Scanning for projects...\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------< \u001b[0;36morg.apache.maven:standalone-pom\u001b[0;1m >-------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1mBuilding Maven Stub Project (No POM) 1\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--------------------------------[ pom ]---------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m>>> \u001b[0;32mmaven-archetype-plugin:3.1.2:generate\u001b[m \u001b[1m(default-cli)\u001b[0;1m > \u001b[0;1mgenerate-sources\u001b[m @ \u001b[36mstandalone-pom\u001b[0;1m >>>\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m<<< \u001b[0;32mmaven-archetype-plugin:3.1.2:generate\u001b[m \u001b[1m(default-cli)\u001b[0;1m < \u001b[0;1mgenerate-sources\u001b[m @ \u001b[36mstandalone-pom\u001b[0;1m <<<\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-archetype-plugin:3.1.2:generate\u001b[m \u001b[1m(default-cli)\u001b[m @ \u001b[36mstandalone-pom\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Generating project in Batch mode\n",
      "[\u001b[1;34mINFO\u001b[m] ----------------------------------------------------------------------------\n",
      "[\u001b[1;34mINFO\u001b[m] Using following parameters for creating project from Old (1.x) Archetype: maven-archetype-quickstart:1.0\n",
      "[\u001b[1;34mINFO\u001b[m] ----------------------------------------------------------------------------\n",
      "[\u001b[1;34mINFO\u001b[m] Parameter: basedir, Value: /Users/khast\n",
      "[\u001b[1;34mINFO\u001b[m] Parameter: package, Value: it.unipi.hadoop\n",
      "[\u001b[1;34mINFO\u001b[m] Parameter: groupId, Value: it.unipi.hadoop\n",
      "[\u001b[1;34mINFO\u001b[m] Parameter: artifactId, Value: wordcount\n",
      "[\u001b[1;34mINFO\u001b[m] Parameter: packageName, Value: it.unipi.hadoop\n",
      "[\u001b[1;34mINFO\u001b[m] Parameter: version, Value: 1.0-SNAPSHOT\n",
      "[\u001b[1;34mINFO\u001b[m] project created from Old (1.x) Archetype in dir: /Users/khast/wordcount\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1;32mBUILD SUCCESS\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Total time:  5.949 s\n",
      "[\u001b[1;34mINFO\u001b[m] Finished at: 2020-04-20T12:29:57+02:00\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mvn archetype:generate -DgroupId=it.unipi.hadoop -DartifactId=wordcount \\\n",
    "                       -DarchetypeArtifactId=maven-archetype-quickstart \\\n",
    "                       -DinteractiveMode=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maven will create a directory named after the provided `artifactId` (in our case, `wordcount`), including a minimal `pom.xml` file and a directory structure like the following:\n",
    "\n",
    "```bash\n",
    "wordcount/src\n",
    "├── main\n",
    "│   └── java\n",
    "│       └── it\n",
    "│           └── unipi\n",
    "│               └── hadoop\n",
    "│                   └── App.java\n",
    "└── test\n",
    "    └── java\n",
    "        └── it\n",
    "            └── unipi\n",
    "                └── hadoop\n",
    "                        └── AppTest.java\n",
    "```\n",
    "\n",
    "For the moment being, we can safely delete/ignore the `test` folder, as well as the `App.java` file. We will write our own Java file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:30:14.157221Z",
     "start_time": "2020-04-20T10:30:14.150316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/khast/wordcount\n"
     ]
    }
   ],
   "source": [
    "%cd wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:30:15.470900Z",
     "start_time": "2020-04-20T10:30:15.341079Z"
    }
   },
   "outputs": [],
   "source": [
    "%rm -rf src/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:30:16.358250Z",
     "start_time": "2020-04-20T10:30:16.228568Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf src/main/java/it/unipi/hadoop/App.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the POM file\n",
    "\n",
    "A **Project Object Model** or **POM** is an XML file that contains information about the project and configuration details used by Maven to build the project. \n",
    "\n",
    "It contains default values for most projects. Examples for this is the build directory, which is `target`; the source directory, which is `src/main/java`; the test source directory, which is `src/test/java`; and so on. \n",
    "\n",
    "Our freshly generated `pom.xml` file is in the `wordcount` directory, and contains the following minimal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:30:19.236545Z",
     "start_time": "2020-04-20T10:30:19.103319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n",
      "  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\r\n",
      "  <modelVersion>4.0.0</modelVersion>\r\n",
      "  <groupId>it.unipi.hadoop</groupId>\r\n",
      "  <artifactId>wordcount</artifactId>\r\n",
      "  <packaging>jar</packaging>\r\n",
      "  <version>1.0-SNAPSHOT</version>\r\n",
      "  <name>wordcount</name>\r\n",
      "  <url>http://maven.apache.org</url>\r\n",
      "  <dependencies>\r\n",
      "    <dependency>\r\n",
      "      <groupId>junit</groupId>\r\n",
      "      <artifactId>junit</artifactId>\r\n",
      "      <version>3.8.1</version>\r\n",
      "      <scope>test</scope>\r\n",
      "    </dependency>\r\n",
      "  </dependencies>\r\n",
      "</project>\r\n"
     ]
    }
   ],
   "source": [
    "%cat pom.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enrich the POM with the elements we need to compile and package an Hadoop program, we need to include some *properties*, *plugins* and *dependencies*. Maven properties are variable that can be re-used in a `pom.xml` file. Plugins are used to create jar files, compile code, unit test code, create project documentation, and on and on. Dependencies are pieces of code (other project, Java libraries, etc) that you code will required to compile and run. Maven atuomatically downloads and links the dependencies on compilation, as well all the the dependencies of those dependencies (transitive dependencies), allowing your list to focus solely on the dependencies your project requires.\n",
    "\n",
    "Your `pom.xml` file must include some properties: the Java compilter versions (Java 8), the project source encoding (UTF-8) and our Hadoop libraries version (3.1.3).\n",
    "\n",
    "```xml\n",
    "<project>\n",
    "  [...]\n",
    "  <properties>\n",
    "    <java.version>1.8</java.version>\n",
    "    <hadoop.version>3.1.3</hadoop.version>\n",
    "    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n",
    "  </properties>\n",
    "  [...]\n",
    "</project>\n",
    "```\n",
    "\n",
    "Your `pom.xml` file must configure the plugins we will use in the build process.\n",
    "\n",
    "```xml\n",
    "<project>\n",
    "  [...]\n",
    "  <build>\n",
    "    <plugins>\n",
    "      <plugin>\n",
    "        <artifactId>maven-compiler-plugin</artifactId>\n",
    "        <version>3.2</version>\n",
    "        <configuration>\n",
    "          <source>${java.version}</source>\n",
    "          <target>${java.version}</target>\n",
    "          <encoding>${project.build.sourceEncoding}</encoding>\n",
    "        </configuration>\n",
    "      </plugin>\n",
    "\n",
    "      <plugin>\n",
    "        <groupId>org.apache.maven.plugins</groupId>\n",
    "        <artifactId>maven-jar-plugin</artifactId>\n",
    "        <version>3.2.0</version>\n",
    "        <configuration>\n",
    "          <archive>\n",
    "            <manifest>\n",
    "              <addClasspath>true</addClasspath>\n",
    "            </manifest>\n",
    "          </archive>\n",
    "        </configuration>\n",
    "      </plugin>\n",
    "    </plugins>\n",
    "  </build>\n",
    "\n",
    " [...]\n",
    "</project>\n",
    "```\n",
    "\n",
    "Your `pom.xml` file must include the Hadoop dependencies. These dependencies must match the Hadoop version installed in your cluster.\n",
    "\n",
    "```xml\n",
    "<project>\n",
    "  [...]\n",
    "  <dependencies>\n",
    "\n",
    "    <dependency>\n",
    "      <groupId>org.apache.hadoop</groupId>\n",
    "      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>\n",
    "      <version>${hadoop.version}</version>\n",
    "    </dependency>\n",
    "    <dependency>\n",
    "      <groupId>org.apache.hadoop</groupId>\n",
    "      <artifactId>hadoop-common</artifactId>\n",
    "      <version>${hadoop.version}</version>\n",
    "    </dependency>\n",
    "    <dependency>\n",
    "      <groupId>org.apache.hadoop</groupId>\n",
    "      <artifactId>hadoop-hdfs-client</artifactId>\n",
    "      <version>${hadoop.version}</version>\n",
    "    </dependency>\n",
    "    <dependency>\n",
    "      <groupId>org.apache.hadoop</groupId>\n",
    "      <artifactId>hadoop-mapreduce-client-app</artifactId>\n",
    "      <version>${hadoop.version}</version>\n",
    "    </dependency>\n",
    "    [...]\n",
    "  </dependencies>\n",
    "  [...]\n",
    "</project>\n",
    "```\n",
    "\n",
    "The following command will populate the `pom.xml` file accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:35:45.891304Z",
     "start_time": "2020-04-20T10:35:45.753988Z"
    }
   },
   "outputs": [],
   "source": [
    "!printf \"%s\\n\" {commands.get_pom()} > pom.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:35:46.785659Z",
     "start_time": "2020-04-20T10:35:46.654069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n",
      "  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\r\n",
      "  <modelVersion>4.0.0</modelVersion>\r\n",
      "  <groupId>it.unipi.hadoop</groupId>\r\n",
      "  <artifactId>wordcount</artifactId>\r\n",
      "  <packaging>jar</packaging>\r\n",
      "  <version>1.0-SNAPSHOT</version>\r\n",
      "  <name>wordcount</name>\r\n",
      "  <url>http://maven.apache.org</url>\r\n",
      "  \r\n",
      "  <properties>\r\n",
      "    <java.version>1.8</java.version>\r\n",
      "    <hadoop.version>3.1.3</hadoop.version>\r\n",
      "    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\r\n",
      "  </properties>\r\n",
      "\r\n",
      "  <build>\r\n",
      "    <plugins>\r\n",
      "      <plugin>\r\n",
      "        <artifactId>maven-compiler-plugin</artifactId>\r\n",
      "        <version>3.2</version>\r\n",
      "        <configuration>\r\n",
      "          <source>${java.version}</source>\r\n",
      "          <target>${java.version}</target>\r\n",
      "          <encoding>${project.build.sourceEncoding}</encoding>\r\n",
      "        </configuration>\r\n",
      "      </plugin>\r\n",
      "\r\n",
      "      <plugin>\r\n",
      "        <groupId>org.apache.maven.plugins</groupId>\r\n",
      "        <artifactId>maven-jar-plugin</artifactId>\r\n",
      "        <version>3.2.0</version>\r\n",
      "        <configuration>\r\n",
      "          <archive>\r\n",
      "            <manifest>\r\n",
      "              <addClasspath>true</addClasspath>\r\n",
      "            </manifest>\r\n",
      "          </archive>\r\n",
      "        </configuration>\r\n",
      "      </plugin>\r\n",
      "    </plugins>\r\n",
      "  </build>\r\n",
      "\r\n",
      "  <dependencies>\r\n",
      "    <dependency>\r\n",
      "      <groupId>junit</groupId>\r\n",
      "      <artifactId>junit</artifactId>\r\n",
      "      <version>3.8.1</version>\r\n",
      "      <scope>test</scope>\r\n",
      "    </dependency>\r\n",
      "    <dependency>\r\n",
      "      <groupId>org.apache.hadoop</groupId>\r\n",
      "      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>\r\n",
      "      <version>${hadoop.version}</version>\r\n",
      "    </dependency>\r\n",
      "    <dependency>\r\n",
      "      <groupId>org.apache.hadoop</groupId>\r\n",
      "      <artifactId>hadoop-common</artifactId>\r\n",
      "      <version>${hadoop.version}</version>\r\n",
      "    </dependency>\r\n",
      "    <dependency>\r\n",
      "      <groupId>org.apache.hadoop</groupId>\r\n",
      "      <artifactId>hadoop-hdfs-client</artifactId>\r\n",
      "      <version>${hadoop.version}</version>\r\n",
      "    </dependency>\r\n",
      "    <dependency>\r\n",
      "      <groupId>org.apache.hadoop</groupId>\r\n",
      "      <artifactId>hadoop-mapreduce-client-app</artifactId>\r\n",
      "      <version>${hadoop.version}</version>\r\n",
      "    </dependency>\r\n",
      "  </dependencies>\r\n",
      "</project>\r\n"
     ]
    }
   ],
   "source": [
    "%cat pom.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command must execute with no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:35:52.826750Z",
     "start_time": "2020-04-20T10:35:48.169691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;34mINFO\u001b[m] Scanning for projects...\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m---------------------< \u001b[0;36mit.unipi.hadoop:wordcount\u001b[0;1m >----------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1mBuilding wordcount 1.0-SNAPSHOT\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--------------------------------[ jar ]---------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-resources-plugin:2.6:resources\u001b[m \u001b[1m(default-resources)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Using 'UTF-8' encoding to copy filtered resources.\n",
      "[\u001b[1;34mINFO\u001b[m] skip non existing resourceDirectory /Users/khast/wordcount/src/main/resources\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-compiler-plugin:3.2:compile\u001b[m \u001b[1m(default-compile)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Changes detected - recompiling the module!\n",
      "[\u001b[1;34mINFO\u001b[m] Compiling 1 source file to /Users/khast/wordcount/target/classes\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1;32mBUILD SUCCESS\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Total time:  2.816 s\n",
      "[\u001b[1;34mINFO\u001b[m] Finished at: 2020-04-20T12:35:52+02:00\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!mvn compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Writing code <a name=\"write\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:03:55.675292Z",
     "start_time": "2020-04-20T10:03:55.648405Z"
    }
   },
   "source": [
    "You can write the source code of your application with any text editor. Here we will use the GNU [`nano`](https://www.nano-editor.org) editor.\n",
    "```bash\n",
    "cd wordcount\n",
    "nano src/main/java/it/unipi/hadoop/WordCount.java\n",
    "```\n",
    "Edit the Java file with content, then close the file (Ctrl+O followed by Ctrl+X).\n",
    "\n",
    "```java\n",
    "package it.unipi.hadoop;\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "import org.apache.hadoop.util.GenericOptionsParser;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key, Text value, Context context\n",
    "                    ) throws IOException, InterruptedException {\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        word.set(itr.nextToken());\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key, Iterable<IntWritable> values,\n",
    "                       Context context\n",
    "                       ) throws IOException, InterruptedException {\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n",
    "    if (otherArgs.length < 2) {\n",
    "      System.err.println(\"Usage: wordcount <in> [<in>...] <out>\");\n",
    "      System.exit(2);\n",
    "    }\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "    job.setJarByClass(WordCount.class);\n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    for (int i = 0; i < otherArgs.length - 1; ++i) {\n",
    "      FileInputFormat.addInputPath(job, new Path(otherArgs[i]));\n",
    "    }\n",
    "    FileOutputFormat.setOutputPath(job,\n",
    "      new Path(otherArgs[otherArgs.length - 1]));\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folder containing your `pom.xml` file, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:36:03.890713Z",
     "start_time": "2020-04-20T10:35:58.815381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;34mINFO\u001b[m] Scanning for projects...\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m---------------------< \u001b[0;36mit.unipi.hadoop:wordcount\u001b[0;1m >----------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1mBuilding wordcount 1.0-SNAPSHOT\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--------------------------------[ jar ]---------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-clean-plugin:2.5:clean\u001b[m \u001b[1m(default-clean)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Deleting /Users/khast/wordcount/target\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-resources-plugin:2.6:resources\u001b[m \u001b[1m(default-resources)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Using 'UTF-8' encoding to copy filtered resources.\n",
      "[\u001b[1;34mINFO\u001b[m] skip non existing resourceDirectory /Users/khast/wordcount/src/main/resources\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-compiler-plugin:3.2:compile\u001b[m \u001b[1m(default-compile)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Changes detected - recompiling the module!\n",
      "[\u001b[1;34mINFO\u001b[m] Compiling 1 source file to /Users/khast/wordcount/target/classes\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-resources-plugin:2.6:testResources\u001b[m \u001b[1m(default-testResources)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Using 'UTF-8' encoding to copy filtered resources.\n",
      "[\u001b[1;34mINFO\u001b[m] skip non existing resourceDirectory /Users/khast/wordcount/src/test/resources\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-compiler-plugin:3.2:testCompile\u001b[m \u001b[1m(default-testCompile)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] No sources to compile\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-surefire-plugin:2.12.4:test\u001b[m \u001b[1m(default-test)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] No tests to run.\n",
      "[\u001b[1;34mINFO\u001b[m] \n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m--- \u001b[0;32mmaven-jar-plugin:3.2.0:jar\u001b[m \u001b[1m(default-jar)\u001b[m @ \u001b[36mwordcount\u001b[0;1m ---\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Building jar: /Users/khast/wordcount/target/wordcount-1.0-SNAPSHOT.jar\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1;32mBUILD SUCCESS\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n",
      "[\u001b[1;34mINFO\u001b[m] Total time:  3.352 s\n",
      "[\u001b[1;34mINFO\u001b[m] Finished at: 2020-04-20T12:36:03+02:00\n",
      "[\u001b[1;34mINFO\u001b[m] \u001b[1m------------------------------------------------------------------------\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!mvn clean package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If compilation and packaging runs smoothly, we will get a new `target` folder, containing the `wordcount-1.0-SNAPSHOT.jar` jar file to use to dispatch our application on any Hadoop cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T10:36:43.319504Z",
     "start_time": "2020-04-20T10:36:43.184584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\r\n",
      "drwxr-xr-x  3 khast  staff    96B Apr 20 12:36 \u001b[1m\u001b[31mmaven-status\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  3 khast  staff    96B Apr 20 12:36 \u001b[1m\u001b[31mgenerated-sources\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  3 khast  staff    96B Apr 20 12:36 \u001b[1m\u001b[31mclasses\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  3 khast  staff    96B Apr 20 12:36 \u001b[1m\u001b[31mmaven-archiver\u001b[m\u001b[m\r\n",
      "-rw-r--r--  1 khast  staff   6.5K Apr 20 12:36 wordcount-1.0-SNAPSHOT.jar\r\n"
     ]
    }
   ],
   "source": [
    "! ls -ltrh target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jar file must be copied to the `namenode-hadoop` virtual machine in the Hadoop cluster.\n",
    "```bash\n",
    "scp target/wordcount-1.0-SNAPSHOT.jar hadoop@<namenode ip address>:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running code <a name=\"deploy\"/>\n",
    "\n",
    "To test the Hadoop program we just wrote, we will use a small input data set called [`pg100.txt`](../exercises/data/pg100.txt) that we have already transferred to the `namenode-hadoop` virtual machine using `scp`\n",
    ".\n",
    "\n",
    "Open a terminal and run the following commands (delete the `output` directory on HDFS if it already exists):\n",
    "```bash\n",
    "hadoop fs -put pg100.txt\n",
    "hadoop jar wordcount-1.0-SNAPSHOT.jar it.unipi.hadoop.WordCount pg100.txt output\n",
    "```\n",
    "\n",
    "Run the following command:\n",
    "```bash\n",
    "hadoop fs -ls output\n",
    "```\n",
    "\n",
    "You should see an output file for each reducer. Since there was only one reducer for this job, you should only see one `part-r-00000` file. Note that sometimes the files will be called `part-00000`, and sometimes they'll be called `part-r-00000`.\n",
    "\n",
    "3. Run the following command:\n",
    "\n",
    "```bash\n",
    "hadoop fs -cat output/part-r-00000 | head\n",
    "```\n",
    "\n",
    "You should see the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. How to run Hadoop programs from your laptop\n",
    "\n",
    "The following notes will describe how to setup your laptop to directly run your Hadoop programs without copying data and JAR files on the `namenode-hadoop` machine.\n",
    "\n",
    "> **I will not cover all the details, and there can be errors. Use the notes at your own risk.**\n",
    "\n",
    "Your laptop must be configured to access the Hadoop cluster and to generated JAR files compatible with the Hadoop cluster. \n",
    "\n",
    "1. Check that the Java VM in your laptop (and `JAVA_HOME`) corresponds **exactly** to the Java VM in your cluster (and `JAVA_HOME`).\n",
    "\n",
    "2. Download on your laptop the Hadoop binary that you installed on your virtual machines. Check that the Hadoop version corresponds **exactly**.\n",
    "\n",
    "3. Unzip the Hadoop tarball wherever you wish, but update your `HADOOP_HOME` and `PATH` env variables accordingly.\n",
    "\n",
    "    ```bash\n",
    "    cd <directory where you unzipped Hadoop>\n",
    "    export HADOOP_HOME=`pwd`\n",
    "    export PATH=$PATH:$HADOOP_HOME/bin\n",
    "    hadoop version\n",
    "    ```\n",
    "\n",
    "    These export will be valid in the current shell only.\n",
    "\n",
    "4. Update the `core-site.xml` file located at `$HADOOP_HOME/etc/hadoop/` to define the name node URI on your laptop.\n",
    "The file must contain the following lines, with the <namenode ip address> updated according to your Hadoop cluster.\n",
    "    ```\n",
    "    <configuration>\n",
    "      <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://<namenode ip address>:9820/</value>\n",
    "      </property>\n",
    "    </configuration>\n",
    "    ```\n",
    "\n",
    "5. You will interact with Hadoop using your **local user**. This means that you need to create suitable directories on HDFS first, such as `/user/foobar/`.\n",
    "\n",
    "Now you are able to interact with HDFS and submit MapReduce jobs from your laptop. Make sure that the Hadoop cluster is up and running with no errors! :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "348.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
