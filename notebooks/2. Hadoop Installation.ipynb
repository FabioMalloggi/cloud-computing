{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Installation and Testing\n",
    "\n",
    "> **WARNING: Be careful, the execution of this notebook can compromise your virtual machines. Do not execute any cell twice: please start from the very first cell if you have problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load commands from `commands.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ssh(host, *command):\n",
    "    \"\"\"\n",
    "    :param host: ip address\n",
    "    :type f: string\n",
    "    :param command: command to execute\n",
    "    :type command: string\n",
    "\n",
    "    Execute with SSH the commands on host as the 'hadoop' user, displaying information\n",
    "    \"\"\"\n",
    "    print('===== \\x1b[31m' + 'Started on ' + host + '\\x1b[0m =====')\n",
    "    for cmd in command:\n",
    "        print(cmd)\n",
    "        !ssh hadoop@{host} {cmd}\n",
    "    print('===== \\x1b[31m' + 'Completed on ' + host + '\\x1b[0m =====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally Hadoop can be run in three modes.\n",
    "\n",
    "1. **Standalone (or local) mode**: There are no daemons used in this mode. Hadoop uses the local file system as a substitute for HDFS file system. The jobs will run as if there is 1 mapper and 1 reducer.\n",
    "2. **Pseudo-distributed mode**: All the daemons run on a single machine and this setting mimics the behavior of a cluster. All the daemons run on your machine locally using the HDFS protocol. There can be multiple mappers and reducers.\n",
    "3. **Fully-distributed mode**: This is how Hadoop runs on a real cluster.\n",
    "\n",
    "In these notes we will describe how to set up an [Hadoop 3](https://hadoop.apache.org) installation to work with.\n",
    "We will set up a *fully-distributed cluster* on your assigned virtual machines.\n",
    "\n",
    "The core of Hadoop is composed by two main subsystem:\n",
    "\n",
    "* the **Hadoop Distributed File System** (HDFS), responsible for the distributed data management\n",
    "* the **Yet Another Resource Negotiator** (YARN), responsible for the distributed code execution\n",
    "\n",
    "Both subsystems are implemented according to the **master-workers** architecture.\n",
    "\n",
    "![Master-workers Architecture](https://raw.githubusercontent.com/mesham/epython/master/docs/masterworker.png)\n",
    "\n",
    "Both HDFS and YARN have their own terminology for master and worker nodes.\n",
    "\n",
    "|    | Master         | Worker      |\n",
    "|:--:|:--------------:|:-----------:|\n",
    "|HDFS| NameNode       | DataNode    |\n",
    "|YARN| ResouceManager | NodeManager |\n",
    "\n",
    "While the masters of HDFS and YARN can, in principle, be located on different machines, we will install the HDFS and YARN masters on a single machine, and install the HDFS and YARN workers on all machines (*including the machine hosting the masters).\n",
    "\n",
    "This notebook contains the steps necessary to set up and configure correctly Hadoop on our virtual machines.\n",
    "In particular:\n",
    "\n",
    "1. We will [download & install](#download) Hadoop on all our virtual machines.\n",
    "2. We will [configure](#namenode) a virtual machine to host the HDFS and YARN masters and workers.\n",
    "3. We will [configure](#datanode) the remaining virtual machines to host the HDFS and YARN workers.\n",
    "4. We will [test](#test) your newly install Hadoop cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites <a name=\"prereq\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Populate the following dictionary with the IP addresses (as keys) and the hostnames (as values) of all your virtual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:55:11.986338Z",
     "start_time": "2020-04-07T21:55:11.980489Z"
    }
   },
   "outputs": [],
   "source": [
    "VMS = {'172.16.0.225': 'hadoop-namenode',\n",
    "       '172.16.0.221': 'hadoop-datanode-2', \n",
    "       '172.16.0.224': 'hadoop-datanode-3'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Populate the following variable with the IP address of the virtual machine with the namenode role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:54:14.685117Z",
     "start_time": "2020-04-07T21:54:14.673997Z"
    }
   },
   "outputs": [],
   "source": [
    "NAMENODE_IP = '172.16.0.225'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Populate the following variable with the IP address of the remaining virtual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMAINING_IPS = [\n",
    "    '172.16.0.221',\n",
    "    '172.16.0.224'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and install Hadoop <a name=\"download\"/>\n",
    "---\n",
    "**a.** Download [hadoop-3.1.3.tar.gz](https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz) in your home folder on your virtual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:44:28.207459Z",
     "start_time": "2020-04-07T19:43:47.090004Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.0.225\u001b[0m =====\n",
      "wget --progress=bar:force -c -O /home/hadoop/hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "--2020-04-16 22:14:35--  https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "Resolving archive.apache.org (archive.apache.org)... 163.172.17.199\n",
      "Connecting to archive.apache.org (archive.apache.org)|163.172.17.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 338075860 (322M) [application/x-gzip]\n",
      "Saving to: ‘/home/hadoop/hadoop.tar.gz’\n",
      "\n",
      "/home/hadoop/hadoop 100%[===================>] 322.41M  47.1MB/s    in 7.8s    \n",
      "\n",
      "2020-04-16 22:14:43 (41.6 MB/s) - ‘/home/hadoop/hadoop.tar.gz’ saved [338075860/338075860]\n",
      "\n",
      "===== \u001b[31mCompleted on 172.16.0.225\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.0.221\u001b[0m =====\n",
      "wget --progress=bar:force -c -O /home/hadoop/hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "--2020-04-16 22:14:44--  https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "Resolving archive.apache.org (archive.apache.org)... 163.172.17.199\n",
      "Connecting to archive.apache.org (archive.apache.org)|163.172.17.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 338075860 (322M) [application/x-gzip]\n",
      "Saving to: ‘/home/hadoop/hadoop.tar.gz’\n",
      "\n",
      "/home/hadoop/hadoop 100%[===================>] 322.41M  51.0MB/s    in 8.8s    \n",
      "\n",
      "2020-04-16 22:14:53 (36.7 MB/s) - ‘/home/hadoop/hadoop.tar.gz’ saved [338075860/338075860]\n",
      "\n",
      "===== \u001b[31mCompleted on 172.16.0.221\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.0.224\u001b[0m =====\n",
      "wget --progress=bar:force -c -O /home/hadoop/hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "--2020-04-16 22:14:55--  https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "Resolving archive.apache.org (archive.apache.org)... 163.172.17.199\n",
      "Connecting to archive.apache.org (archive.apache.org)|163.172.17.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 338075860 (322M) [application/x-gzip]\n",
      "Saving to: ‘/home/hadoop/hadoop.tar.gz’\n",
      "\n",
      "/home/hadoop/hadoop 100%[===================>] 322.41M  51.9MB/s    in 6.6s    \n",
      "\n",
      "2020-04-16 22:15:02 (48.6 MB/s) - ‘/home/hadoop/hadoop.tar.gz’ saved [338075860/338075860]\n",
      "\n",
      "===== \u001b[31mCompleted on 172.16.0.224\u001b[0m =====\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, commands.WGET_CMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Decompress the Hadoop package you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:55:22.045210Z",
     "start_time": "2020-04-07T21:55:14.894132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.0.225\u001b[0m =====\n",
      "tar -xvf hadoop.tar.gz --directory=/opt/hadoop --exclude=hadoop-3.1.0/share/doc --strip 1 > /dev/null\n",
      "rm /home/hadoop/hadoop.tar.gz\n",
      "===== \u001b[31mCompleted on 172.16.0.225\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.0.221\u001b[0m =====\n",
      "tar -xvf hadoop.tar.gz --directory=/opt/hadoop --exclude=hadoop-3.1.0/share/doc --strip 1 > /dev/null\n",
      "rm /home/hadoop/hadoop.tar.gz\n",
      "===== \u001b[31mCompleted on 172.16.0.221\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.0.224\u001b[0m =====\n",
      "tar -xvf hadoop.tar.gz --directory=/opt/hadoop --exclude=hadoop-3.1.0/share/doc --strip 1 > /dev/null\n",
      "rm /home/hadoop/hadoop.tar.gz\n",
      "===== \u001b[31mCompleted on 172.16.0.224\u001b[0m =====\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, commands.TAR_CMD, commands.RM_CMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** There are environment settings that will be used by Hadoop. The following commands append the correct environment variables to your `/home/hadoop/.bashrc` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:55:32.077891Z",
     "start_time": "2020-04-07T21:55:28.999693Z"
    }
   },
   "outputs": [],
   "source": [
    "for host in VMS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_bashrc()} >> ~/.bashrc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** The following commands check Hadoop installation (you should see no errors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:55:37.983835Z",
     "start_time": "2020-04-07T21:55:33.556465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.0.225\u001b[0m =====\n",
      "hadoop version\n",
      "Hadoop 3.1.3\n",
      "Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579\n",
      "Compiled by ztang on 2019-09-12T02:47Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum ec785077c385118ac91aadde5ec9799\n",
      "This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar\n",
      "===== \u001b[31mCompleted on 172.16.0.225\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.0.221\u001b[0m =====\n",
      "hadoop version\n",
      "Hadoop 3.1.3\n",
      "Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579\n",
      "Compiled by ztang on 2019-09-12T02:47Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum ec785077c385118ac91aadde5ec9799\n",
      "This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar\n",
      "===== \u001b[31mCompleted on 172.16.0.221\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.0.224\u001b[0m =====\n",
      "hadoop version\n",
      "Hadoop 3.1.3\n",
      "Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579\n",
      "Compiled by ztang on 2019-09-12T02:47Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum ec785077c385118ac91aadde5ec9799\n",
      "This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar\n",
      "===== \u001b[31mCompleted on 172.16.0.224\u001b[0m =====\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, 'hadoop version')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure the `hadoop-namenode` machine <a name=\"namenode\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Update the `core-site.xml` file located at `/opt/hadoop/etc/hadoop/` to define the name node URI on this machine.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop-namenode:9820/</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:07:53.169774Z",
     "start_time": "2020-04-07T22:07:52.007599Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_namenode_core_site(VMS[NAMENODE_IP])} > /opt/hadoop/etc/hadoop/core-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Update the `hdfs-site.xml` file located at `/opt/hadoop/etc/hadoop/` to define the path on the local filesystem where the name node stores the namespace and transactions logs persistently and to configure the HDFS subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>file:///opt/hdfs/namenode</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>file:///opt/hdfs/datanode</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.permissions</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.use.datanode.hostname</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:10:07.811788Z",
     "start_time": "2020-04-07T22:10:03.228836Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_namenode_hdfs_site()} > /opt/hadoop/etc/hadoop/hdfs-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Update the `yarn-site.xml` file located at `/opt/hadoop/etc/hadoop` to configure the YARN subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n",
    "    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.local-dirs</name>\n",
    "    <value>file:///opt/yarn/local</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.log-dirs</name>\n",
    "    <value>file:///opt/yarn/logs</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "</configuration>```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:11:08.241162Z",
     "start_time": "2020-04-07T22:11:07.094038Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_namenode_yarn_site()} > /opt/hadoop/etc/hadoop/yarn-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Update the `mapred-site.xml` file located at `/opt/hadoop/etc/hadoop` to configure the MAPREDUCE subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.jobhistory.address</name>\n",
    "    <value>{namenode_hostname}:10020</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.jobhistory.webapp.address</name>\n",
    "    <value>{namenode_hostname}:19888</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.jobhistory.intermediate-done-dir</name>\n",
    "    <value>/mr-history/tmp</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.jobhistory.done-dir</name>\n",
    "    <value>/mr-history/done</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.app.mapreduce.am.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.map.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.reduce.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "  </property>\n",
    "</configuration>```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:11:10.387867Z",
     "start_time": "2020-04-07T22:11:09.326366Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_namenode_mapred_site(VMS[NAMENODE_IP])} > /opt/hadoop/etc/hadoop/mapred-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If your machines have more than 2 GB of RAM or if you are interested in the numbers we specified in the YARN and MAPRED configuration files, check the Append A on the [Hadoop Memory Allocaltion](#memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.** Update the `workers` file located in `/opt/hadoop/etc/hadoop` to define the MAPREDUCE workers.\n",
    "With out virtual machines listed [here](#prereq), the file must contain the following lines.\n",
    "```\n",
    "172.16.0.225\n",
    "172.16.0.221\n",
    "172.16.0.224\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:17:40.521472Z",
     "start_time": "2020-04-07T22:17:39.217530Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_workers(VMS)} > /opt/hadoop/etc/hadoop/workers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure the `hadoop-datanode` machines <a name=\"datanode\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Update the `core-site.xml` file located at `/opt/hadoop/etc/hadoop/` to define the name node URI on thie other datanodes.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop-namenode:9820/</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host in REMAINING_IPS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_datanode_core_site(VMS[NAMENODE_IP])} > /opt/hadoop/etc/hadoop/core-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Update the `hdfs-site.xml` file located at `/opt/hadoop/etc/hadoop/` to configure the HDFS subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>file:///opt/hdfs/datanode</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.permissions</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.use.datanode.hostname</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host in REMAINING_IPS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_datanode_hdfs_site()} > /opt/hadoop/etc/hadoop/hdfs-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Update the `yarn-site.xml` file located at `/opt/hadoop/etc/hadoop` to configure the YARN subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host in REMAINING_IPS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_datanode_yarn_site()} > /opt/hadoop/etc/hadoop/yarn-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Update the `mapred-site.xml` file located at `/opt/hadoop/etc/hadoop` to configure the MAPREDUCE subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.app.mapreduce.am.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.map.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.reduce.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host in REMAINING_IPS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_datanode_mapred_site()} > /opt/hadoop/etc/hadoop/mdapred-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If your machines have more than 2 GB of RAM or if you are interested in the numbers we specified in the YARN and MAPRED configuration files, check the Append A on the [Hadoop Memory Allocaltion](#memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start, test and stop Hadoop <a name=\"test\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From now, all commands will be issued from the `hadoop-namenode` virtual machine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a**. Delete the contents of the local HDFS file system.\n",
    "Note: **This causes the loss of all information stored in HDFS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [hadoop-namenode]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [hadoop-namenode]\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, 'rm -rf /opt/hdfs/namenode/*')\n",
    "    run_ssh(host, 'rm -rf /opt/hdfs/datanode/*')\n",
    "    \n",
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'stop-dfs.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Format the HDFS filesystem at the namenode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-16 22:16:54,966 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = hadoop-namenode/172.16.0.225\n",
      "STARTUP_MSG:   args = [-format, -force]\n",
      "STARTUP_MSG:   version = 3.1.3\n",
      "STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.4.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.1.3.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.7.8.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.1.3.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.1.3-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.1.3-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.3-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.1.3-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.3-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.3.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.1.3.jar\n",
      "STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579; compiled by 'ztang' on 2019-09-12T02:47Z\n",
      "STARTUP_MSG:   java = 1.8.0_242\n",
      "************************************************************/\n",
      "2020-04-16 22:16:55,050 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-16 22:16:55,240 INFO namenode.NameNode: createNameNode [-format, -force]\n",
      "Formatting using clusterid: CID-88963bcc-895b-498d-9d9e-8bfde3833e23\n",
      "2020-04-16 22:16:57,066 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2020-04-16 22:16:57,117 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2020-04-16 22:16:57,119 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2020-04-16 22:16:57,125 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2020-04-16 22:16:57,130 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)\n",
      "2020-04-16 22:16:57,131 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "2020-04-16 22:16:57,131 INFO namenode.FSNamesystem: isPermissionEnabled = false\n",
      "2020-04-16 22:16:57,131 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2020-04-16 22:16:57,223 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2020-04-16 22:16:57,251 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "2020-04-16 22:16:57,251 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2020-04-16 22:16:57,256 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2020-04-16 22:16:57,256 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Apr 16 22:16:57\n",
      "2020-04-16 22:16:57,258 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2020-04-16 22:16:57,258 INFO util.GSet: VM type       = 64-bit\n",
      "2020-04-16 22:16:57,267 INFO util.GSet: 2.0% max memory 1.7 GB = 34.5 MB\n",
      "2020-04-16 22:16:57,267 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
      "2020-04-16 22:16:57,280 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2020-04-16 22:16:57,290 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "2020-04-16 22:16:57,290 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "2020-04-16 22:16:57,290 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2020-04-16 22:16:57,290 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2020-04-16 22:16:57,290 INFO blockmanagement.BlockManager: defaultReplication         = 2\n",
      "2020-04-16 22:16:57,290 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2020-04-16 22:16:57,290 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2020-04-16 22:16:57,290 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2020-04-16 22:16:57,291 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2020-04-16 22:16:57,291 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2020-04-16 22:16:57,291 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2020-04-16 22:16:57,377 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\n",
      "2020-04-16 22:16:57,407 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2020-04-16 22:16:57,407 INFO util.GSet: VM type       = 64-bit\n",
      "2020-04-16 22:16:57,407 INFO util.GSet: 1.0% max memory 1.7 GB = 17.3 MB\n",
      "2020-04-16 22:16:57,407 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "2020-04-16 22:16:57,434 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "2020-04-16 22:16:57,434 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2020-04-16 22:16:57,434 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2020-04-16 22:16:57,434 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2020-04-16 22:16:57,440 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2020-04-16 22:16:57,443 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2020-04-16 22:16:57,458 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2020-04-16 22:16:57,458 INFO util.GSet: VM type       = 64-bit\n",
      "2020-04-16 22:16:57,459 INFO util.GSet: 0.25% max memory 1.7 GB = 4.3 MB\n",
      "2020-04-16 22:16:57,459 INFO util.GSet: capacity      = 2^19 = 524288 entries\n",
      "2020-04-16 22:16:57,477 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2020-04-16 22:16:57,477 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2020-04-16 22:16:57,477 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2020-04-16 22:16:57,484 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2020-04-16 22:16:57,484 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2020-04-16 22:16:57,488 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2020-04-16 22:16:57,488 INFO util.GSet: VM type       = 64-bit\n",
      "2020-04-16 22:16:57,488 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 530.4 KB\n",
      "2020-04-16 22:16:57,488 INFO util.GSet: capacity      = 2^16 = 65536 entries\n",
      "2020-04-16 22:16:57,620 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1117530589-172.16.0.225-1587071817607\n",
      "2020-04-16 22:16:57,709 INFO common.Storage: Storage directory /opt/hdfs/namenode has been successfully formatted.\n",
      "2020-04-16 22:16:57,753 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2020-04-16 22:16:57,897 INFO namenode.FSImageFormatProtobuf: Image file /opt/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 390 bytes saved in 0 seconds .\n",
      "2020-04-16 22:16:57,923 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2020-04-16 22:16:57,945 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\n",
      "2020-04-16 22:16:57,948 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at hadoop-namenode/172.16.0.225\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'hdfs namenode -format -force'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Creating the HDFS home folder for the `hadoop` user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [hadoop-namenode]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [hadoop-namenode]\n",
      "Stopping namenodes on [hadoop-namenode]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [hadoop-namenode]\n"
     ]
    }
   ],
   "source": [
    "!ssh hadoop@{host} 'start-dfs.sh'\n",
    "!ssh hadoop@{host} 'hadoop fs -mkdir -p /user/hadoop'\n",
    "!ssh hadoop@{host} 'stop-dfs.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d**. Starting HDFS & YARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [hadoop-namenode]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [hadoop-namenode]\n",
      "Starting resourcemanager\n",
      "Starting nodemanagers\n"
     ]
    }
   ],
   "source": [
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'start-dfs.sh'\n",
    "!ssh hadoop@{host} 'start-yarn.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e**. Checking all daemons up and running. You should receive 6/7 lines from the namenode machne and 3 lines from the datanote machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.0.225\u001b[0m =====\n",
      "jps\n",
      "219747 SecondaryNameNode\n",
      "219489 DataNode\n",
      "220262 NodeManager\n",
      "3000 HistoryServer\n",
      "220059 ResourceManager\n",
      "220699 Jps\n",
      "219263 NameNode\n",
      "===== \u001b[31mCompleted on 172.16.0.225\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.0.221\u001b[0m =====\n",
      "jps\n",
      "154352 Jps\n",
      "153968 DataNode\n",
      "154154 NodeManager\n",
      "===== \u001b[31mCompleted on 172.16.0.221\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.0.224\u001b[0m =====\n",
      "jps\n",
      "197748 Jps\n",
      "197364 DataNode\n",
      "197550 NodeManager\n",
      "===== \u001b[31mCompleted on 172.16.0.224\u001b[0m =====\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, 'jps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may check logs at `/opt/hadoop/logs` on the 3 machines and check if everything is alright, or running the `hdfs dfsadmin -report` command (it must return `Live datanodes (3)`).\n",
    "\n",
    "You can access Hadoop on a browser on your local machine (use IP addresses, not hostnames):\n",
    "- HDFS subsystem: `http://172.16.0.225:9870/`\n",
    "- YARN subsystem: `http://172.16.0.1:8088/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f.** Run an example provided by Hadoop.\n",
    "* Wait a minute before running, the daemons should perform some initialization steps\n",
    "* Ignore initial errors `No such file or directory`\n",
    "* Ignore logger message by logger `sasl.SaslDataTransferClient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `input': No such file or directory\n",
      "rm: `output': No such file or directory\n",
      "2020-04-16 22:17:57,402 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,303 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,366 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,602 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,670 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,718 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,773 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,832 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,883 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:58,936 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,001 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,055 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,114 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,183 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,248 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,311 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,380 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,437 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,493 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,554 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,614 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,683 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,731 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,796 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,859 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,929 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:17:59,987 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:00,058 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:00,110 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:00,175 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:00,226 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:00,292 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:03,197 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-04-16 22:18:03,637 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587071865624_0001\n",
      "2020-04-16 22:18:03,751 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:03,940 INFO input.FileInputFormat: Total input files to process : 9\n",
      "2020-04-16 22:18:04,057 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:04,155 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:04,187 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2020-04-16 22:18:05,209 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:05,244 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587071865624_0001\n",
      "2020-04-16 22:18:05,244 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-04-16 22:18:05,441 INFO conf.Configuration: resource-types.xml not found\n",
      "2020-04-16 22:18:05,441 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2020-04-16 22:18:05,682 INFO impl.YarnClientImpl: Submitted application application_1587071865624_0001\n",
      "2020-04-16 22:18:05,736 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1587071865624_0001/\n",
      "2020-04-16 22:18:05,736 INFO mapreduce.Job: Running job: job_1587071865624_0001\n",
      "2020-04-16 22:18:13,856 INFO mapreduce.Job: Job job_1587071865624_0001 running in uber mode : false\n",
      "2020-04-16 22:18:13,857 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-04-16 22:18:22,948 INFO mapreduce.Job:  map 22% reduce 0%\n",
      "2020-04-16 22:18:23,954 INFO mapreduce.Job:  map 44% reduce 0%\n",
      "2020-04-16 22:18:28,991 INFO mapreduce.Job:  map 56% reduce 0%\n",
      "2020-04-16 22:18:32,019 INFO mapreduce.Job:  map 89% reduce 0%\n",
      "2020-04-16 22:18:36,040 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2020-04-16 22:18:36,048 INFO mapreduce.Job: Job job_1587071865624_0001 completed successfully\n",
      "2020-04-16 22:18:36,135 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=186\n",
      "\t\tFILE: Number of bytes written=2182085\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28460\n",
      "\t\tHDFS: Number of bytes written=302\n",
      "\t\tHDFS: Number of read operations=32\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=114974\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=12202\n",
      "\t\tTotal time spent by all map tasks (ms)=57487\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6101\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=57487\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=6101\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=14716672\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1561856\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=773\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=168\n",
      "\t\tMap output materialized bytes=234\n",
      "\t\tInput split bytes=1131\n",
      "\t\tCombine input records=6\n",
      "\t\tCombine output records=6\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=234\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=1498\n",
      "\t\tCPU time spent (ms)=4670\n",
      "\t\tPhysical memory (bytes) snapshot=2584981504\n",
      "\t\tVirtual memory (bytes) snapshot=19656232960\n",
      "\t\tTotal committed heap usage (bytes)=1965031424\n",
      "\t\tPeak Map Physical memory (bytes)=271380480\n",
      "\t\tPeak Map Virtual memory (bytes)=1966092288\n",
      "\t\tPeak Reduce Physical memory (bytes)=165437440\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1972895744\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=27329\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-16 22:18:36,159 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "2020-04-16 22:18:36,197 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1587071865624_0002\n",
      "2020-04-16 22:18:36,233 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:36,304 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2020-04-16 22:18:36,335 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:36,392 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:36,417 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2020-04-16 22:18:36,453 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2020-04-16 22:18:36,494 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1587071865624_0002\n",
      "2020-04-16 22:18:36,494 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2020-04-16 22:18:36,516 INFO impl.YarnClientImpl: Submitted application application_1587071865624_0002\n",
      "2020-04-16 22:18:36,526 INFO mapreduce.Job: The url to track the job: http://hadoop-namenode:8088/proxy/application_1587071865624_0002/\n",
      "2020-04-16 22:18:36,526 INFO mapreduce.Job: Running job: job_1587071865624_0002\n",
      "2020-04-16 22:18:47,639 INFO mapreduce.Job: Job job_1587071865624_0002 running in uber mode : false\n",
      "2020-04-16 22:18:47,639 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2020-04-16 22:18:51,665 INFO mapreduce.Job:  map 100% reduce 0%\n"
     ]
    }
   ],
   "source": [
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'hadoop fs -rm -r input output'\n",
    "!ssh hadoop@{host} 'hadoop fs -put /opt/hadoop/etc/hadoop/ input'\n",
    "!ssh hadoop@{host} \"hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar grep /user/hadoop/input/*.xml /user/hadoop/output 'dfs[a-z.]+'\"\n",
    "!ssh hadoop@{host} 'hadoop fs -cat output/part-r-00000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g.** Stop HDFS & YARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'stop-dfs.sh'\n",
    "!ssh hadoop@{host} 'stop-yarn.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If you get an error like the following:\n",
    "```bash\n",
    "[2020-01-14 08:48:28.567]Container [pid=155967,containerID=container_1578991625193_0002_01_000023] is running 380426752B beyond the 'VIRTUAL' memory limit. Current usage: 151.6 MB of 1 GB physical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing container.\n",
    "```\n",
    "you are using more virtual memory than your current limit of 2.1 Gb. This can be resolved in two ways:\n",
    "\n",
    "  1. **Disable Virtual Memory Limit Checking**<br>YARN will simply ignore the limit; in order to do this, add this to your `yarn-site.xml` _on each machine_:\n",
    "      ```bash\n",
    "      <property>\n",
    "        <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "        <value>false</value>\n",
    "      </property>\n",
    "      ```\n",
    "      The default for this setting is `true`.\n",
    "\n",
    "  2. **Increase Virtual Memory to Physical Memory Ratio**<br>In your `yarn-site.xml` change this to a higher value than is currently set, _on each machine_:\n",
    "      ```bash\n",
    "      <property>\n",
    "        <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
    "        <value>5</value>\n",
    "      </property>\n",
    "      ```\n",
    "      The default is 2.1.<br>\n",
    "      You could also increase the amount of physical memory you allocate to a container.<br>\n",
    "      _Make sure you don't forget to restart yarn after you change the configuration_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A. Understanding the Hadoop Memory Allocation <a name=\"memory\"/>\n",
    "\n",
    "Memory allocation can be complex on low RAM nodes because default values are not suitable for nodes with less than 8 GB of RAM.\n",
    "In this section we highlight how memory allocation works for MapReduce jobs, and provide a sample configuration for 2 GB RAM nodes.\n",
    "\n",
    "A YARN job is executed with two kind of resources:\n",
    "\n",
    "* An **application master** (AM), which is responsible for monitoring the application and coordinating distributed executors in the cluster.\n",
    "* Some **executors**, that are created by the AM to actually run the job. For a MapReduce job, they will perform map or reduce operation, in parallel.\n",
    "\n",
    "Both are run in **containers** on **worker nodes**. Each worker node runs a **NodeManager** daemon that is responsible for container creation on the node.\n",
    "The whole cluster is managed by a **ResourceManager** that schedules container allocation on all the worker nodes, depending on capacity requirements and current charge.\n",
    "\n",
    "Four types of resource allocations need to be configured properly for the cluster to work. These are:\n",
    "\n",
    "1. _How much memory can be allocated for YARN containers on a single node._   This limit should be higher than all the others; otherwise, container allocation will be rejected and applications will fail. However, it should not be the entire amount of RAM on the node.\n",
    "This value is configured in the `yarn-site.xml` file with the `yarn.nodemanager.resource.memory-mb` property.\n",
    "\n",
    "2. _How much memory a single container can consume and the minimum memory allocation allowed._\n",
    "A container will never be bigger than the maximum, or else allocation will fail and will always be allocated as a multiple of the minimum amount of RAM.\n",
    "Those values are configured in the `yarn-site.xml` file with the `yarn.scheduler.maximum-allocation-mb` and `yarn.scheduler.minimum-allocation-mb` properties.\n",
    "\n",
    "3. _How much memory will be allocated to the ApplicationMaster._\n",
    "This is a constant value that should fit in the container maximum size.\n",
    "This value is configured in the `mapred-site.xml` with the `yarn.app.mapreduce.am.resource.mb` property.\n",
    "\n",
    "4. _How much memory will be allocated to each map or reduce operation._\n",
    "This should be less than the maximum size.\n",
    "This value is configured in the `mapred-site.xml` file with the `mapreduce.map.memory.mb` and `mapreduce.reduce.memory.mb` properties.\n",
    "\n",
    "The relationship between all those properties can be seen in the following figure:\n",
    "![](https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/hadoop-2-memory-allocation-new.png)\n",
    "\n",
    "For 2 GB nodes, a working configuration may be:\n",
    "\n",
    "|Property|Value|\n",
    "|:-------|:----|\n",
    "|`yarn.nodemanager.resource.memory-mb`  | 1536|\n",
    "|`yarn.scheduler.maximum-allocation-mb` | 1536|\n",
    "|`yarn.scheduler.minimum-allocation-mb` |  128|\n",
    "|`yarn.app.mapreduce.am.resource.mb`    |  512|\n",
    "|`mapreduce.map.memory.mb`              |  256|\n",
    "|`mapreduce.reduce.memory.mb`           |  256|\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
